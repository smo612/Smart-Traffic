import torch
import numpy as np
import matplotlib.pyplot as plt
from dqn_agent import DQNAgent
from rl_env import TrafficRLEnv
from config import TRAFFIC_PATTERN
import sys

# âœ… è¨“ç·´åƒæ•¸èª¿æ•´
EPISODES = 30000
TEST_INTERVAL = 500

# âœ… è¼¸å‡ºç›®å‰ traffic pattern
print(f"ğŸš¦ ä½¿ç”¨äº¤é€šæ¨¡å¼ï¼š{TRAFFIC_PATTERN}")

env = TrafficRLEnv()
state_dim = env.state_dim
action_dim = env.action_dim
agent = DQNAgent(state_dim, action_dim)

train_rewards = []
test_rewards = []
best_reward = -float("inf")

print("ğŸ§  é–‹å§‹è¨“ç·´ DQN æ¨¡å‹... (Ctrl+C å¯ä¸­æ–·ä¸¦è‡ªå‹•å„²å­˜èˆ‡ç•«åœ–)")

try:
    for ep in range(EPISODES):
        state = env.reset()
        total_reward = 0

        for _ in range(300):
            action = agent.select_action(state)
            next_state, reward, done = env.step(action)

            # âœ… å¼·åŒ–é€šè»Šçå‹µã€åŠ å…¥ reward clip
            if reward != 0:
                reward = np.clip(reward, -1000, 1000)

            agent.store((state, action, reward, next_state, done))
            agent.train_step()
            agent.soft_update()

            state = next_state
            total_reward += reward

        agent.decay_epsilon()
        train_rewards.append(total_reward)

        print(f"[Train] Ep {ep}, Reward = {total_reward:.2f}, Epsilon = {agent.epsilon:.3f}")

        if ep % TEST_INTERVAL == 0:
            test_env = TrafficRLEnv(test_mode=True)
            state = test_env.reset()
            test_total_reward = 0

            for _ in range(300):
                action = agent.select_action(state)
                next_state, reward, _ = test_env.step(action)
                test_total_reward += reward
                state = next_state

            test_rewards.append(test_total_reward)
            print(f"[Test] Ep {ep} | Reward = {test_total_reward:.2f}")

            if test_total_reward > best_reward:
                best_reward = test_total_reward
                torch.save(agent.policy_net.state_dict(), "results/best_model.pth")
                print(f"âœ… Best model updated at Ep {ep} | Reward = {test_total_reward:.2f}")

except KeyboardInterrupt:
    print("ğŸ›‘ æ‰‹å‹•ä¸­æ–·è¨“ç·´ï¼Œå„²å­˜æˆæœä¸­...")

finally:
    # å„²å­˜æ¨¡å‹èˆ‡è¨“ç·´è³‡æ–™
    torch.save(agent.policy_net.state_dict(), "results/last_model.pth")
    np.save("results/train_rewards.npy", np.array(train_rewards))
    np.save("results/test_rewards.npy", np.array(test_rewards))

    # ç•« reward æ›²ç·šåœ–
    plt.figure()
    plt.plot(train_rewards, label="Train Reward")
    plt.plot(range(0, len(test_rewards) * TEST_INTERVAL, TEST_INTERVAL), test_rewards, label="Test Reward")
    plt.xlabel("Episode")
    plt.ylabel("Reward")
    plt.title("Training vs Test Reward")
    plt.legend()
    plt.savefig("results/reward_curve.png")
    plt.close()

    print("âœ… å·²å„²å­˜ last_model.pth èˆ‡ reward_curve.png")
